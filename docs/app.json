[{"name": "app.py", "content": "from model_1 import MediaModel\nfrom shiny import App, reactive, render, ui\n\n\ndef panel_box(*args, **kwargs):\n    return ui.div(\n        ui.div(*args, class_=\"card-body\"),\n        **kwargs,\n        class_=\"card mb-3\",\n    )\n\n\napp_ui = ui.page_fluid(\n    {\"class\": \"p-4\"},\n    ui.row(\n        ui.column(\n            10,\n            \"Google Search Volumes Predictor\",\n            panel_box(\n                ui.input_slider(\n                    \"strength\", \"Retention Factor\", min=0, max=1, value=0.5, step=0.1\n                ),\n                ui.input_slider(\"lenght\", \"Weeks\", min=0, max=5, value=3, step=1),\n            ),\n            ui.navset_tab_card(\n                ui.nav(\n                    \"Model 1\",\n                    ui.column(\n                        10,\n                        \"With the selected values we have the following fit graph:\",\n                        ui.output_plot(\"fit_plot\", width=\"700px\", height=\"500px\"),\n                    ), \n                    ui.column(\n                        10,\n                        \"Results obtained in each fold during cross validation process:\",\n                        ui.output_table(\"crossval_table\"),\n                        ui.output_text_verbatim(\"txt_crossval\") \n                    ),\n                    ui.column(\n                        10,\n                        \"Efficiency measure (ROI):\",\n                        ui.output_table(\"generate_roi_table\"), \n                    ),\n                    ui.column(\n                        10,\n                        \"Campaign contribution to the search volume:\", # No se ve\n                        ui.output_plot(\"camp_contribution_plot\", width=\"700px\", height=\"500px\"),\n                    ),\n                ),\n                ui.nav(\n                    \"Model 2\",\n                    ui.column(\n                        10,\n                        #ui.output_plot(\"contr_plot\", width=\"700px\", height=\"500px\"),\n                    ),\n                ),\n            ),\n        ),\n        # ui.column(\n        #     8,\n        #     #ui.output_text_verbatim(\"txt\"),\n        #     ui.output_plot(\"fit_plot\", width=\"500px\", height=\"500px\"),\n        #     ui.output_table(\"generate_roi_table\"),\n        # ),\n    ),\n)\n\n\ndef server(input, output, session):\n    \n    @output\n    @render.plot\n    def fit_plot(): \n        medmod = MediaModel(input.strength(), input.lenght(), a = 0.5)\n        df, X, y = medmod.prepare_data()\n        coef, intercept, y_pred = medmod.predict(X, y)\n        fig = medmod.make_fit_plot(X, y, y_pred)\n        return fig\n\n    @output\n    @render.table\n    def crossval_table():\n        medmod = MediaModel(input.strength(), input.lenght(), a = 0.5)\n        df, X, y = medmod.prepare_data()\n        cv_df, cv_r2_mean, cv_n_1_r2_mean = medmod.get_crossval_table(X, y)\n        return (\n            cv_df.style.set_table_attributes(\n                'class=\"dataframe shiny-table table w-auto\"'\n            ).hide(axis=\"index\")\n             .format(\n                {\n                    \"fit_time\": \"{0:0.4f}\",\n                    \"score_time\": \"{0:0.4f}\",\n                    \"test_r2\": \"{0:0.4f}\",\n                    \"test_neg_mean_squared_error\": \"{0:0.4f}\"\n                }\n             )\n        )\n\n    \n    @output\n    @render.text\n    def txt_crossval():\n        medmod = MediaModel(input.strength(), input.lenght(), a = 0.5)\n        df, X, y = medmod.prepare_data()\n        cv_df, cv_r2_mean, cv_n_1_r2_mean = medmod.get_crossval_table(X, y)\n        return f\"The mean of the r2 is {cv_r2_mean} and if the last fold is discarded the mean is {cv_n_1_r2_mean}.\"\n\n    \n    @output\n    @render.table\n    def generate_roi_table(): \n        medmod = MediaModel(input.strength(), input.lenght(), a = 0.5)\n        df, X, y = medmod.prepare_data()\n        coef, intercept, y_pred = medmod.predict(X, y)\n        adj_contributions = medmod.calculate_roi(X, y, coef, intercept)\n        roi_df = medmod.roi_table(df, adj_contributions)\n        return (\n            roi_df.style.set_table_attributes(\n                'class=\"dataframe shiny-table table w-auto\"'\n            ).hide(axis=\"index\")\n             .format(\n                {\n                    \"campaign_n\": \"{0:0.0f}\",\n                    \"searches_from_camp_n\": \"{0:0.2f}\",\n                    \"spendings_on_camp_n\": \"{0:0.0f}\",\n                    \"roi\": \"{0:0.3f}\",\n                    \"1/roi\": \"{0:0.2f}\",\n                }\n             )\n        )\n\n\n    @output\n    @render.plot\n    def camp_contribution_plot(): # No se ve\n        medmod = MediaModel(input.strength(), input.lenght(), a = 0.5)\n        df, X, y = medmod.prepare_data()\n        coef, intercept, y_pred = medmod.predict(X, y)\n        adj_contributions = medmod.calculate_roi(X, y, coef, intercept)\n        fig2 = medmod.contribution_plot(adj_contributions)\n        return fig2\n\n\napp = App(app_ui, server, debug=True)", "type": "text"}, {"name": "README.md", "content": "# Recruitment Candidate Exercise\nExercise for Solutions Analytics Director role\n\nThis exercise aims to gain an understanding of how a brand\u2019s advertising spend has influenced the levels of weekly Google Search volumes that were made for the brand in a particular country.\n\nIn this case, the advertising has specifically intended to increase in Search volumes and over time three different (non-overlapping) advertising campaigns have been used.\n\nData for the weekly Search volumes; the advertising spend; and where the three different campaigns take place are available.\n\nThe task is to create two different modelling approaches for the data with Search Volume as the dependent. The aim is use the media spend to gain an understanding of which of the campaigns appear to have more effectively and efficiently generated additional Search volumes.\n\nBecause the media/advertising spend will have an impact in the week in which it takes place and a decaying effect in future weeks, it is necessary to represent the media spend in the model in the form of a 'recent advertising pressure' measure. This type of measure with media spend is called an Adstock; and it is in this form that the advertising should be used as an independent variable in the model. The Adstock calculation takes the form:\n\nAdstock (in week n) = Media Spend (in week n) + [ RF x Adstock (in week n-1) ]\n\nThe RF is the Retention Factor [0,1] describing the proportion of the media pressure the is carried over from week to week.\n\nResults for the two modelled approaches should be delivered as a Shiny App.\n\nThere should be a slider to allow the viewer to alter the value of the RF (in increments of 0.1); and as well as a chart showing the model fit, there should also be a table that reports the efficiencies for the three campaigns.\n\nThe models don\u2019t need to be complicated and the Shiny app UI should be simple. Templated Shiny UI is enough. We expect that each model will use its own function with appropriate documentation and that the code will be pushed to your Github repository. We expect to see at least two commits. Along with pushing your code to Github, you should deploy your Shiny app on the Shiny server (using a free account https://www.shinyapps.io/).\n\nThe data for the exercise are available here https://github.com/schubertjan/recruitmentCandidateExercise. You are expected to fork the repository into your own Github account and make any code commits in this forked repository.\n", "type": "text"}, {"name": "data.csv", "content": "\ufeffDate (Week),Media Spend (USD),Media Campaign,Search Volume\n6-Jan-14,0,1,17\n13-Jan-14,0,1,18\n20-Jan-14,0,1,18\n27-Jan-14,11,1,16\n3-Feb-14,32,1,18\n10-Feb-14,12,1,16\n17-Feb-14,16,1,16\n24-Feb-14,17,1,19\n3-Mar-14,19,1,16\n10-Mar-14,26,1,24\n17-Mar-14,32,1,23\n24-Mar-14,12,1,16\n31-Mar-14,32,1,23\n7-Apr-14,54,1,26\n14-Apr-14,84,1,31\n21-Apr-14,68,1,29\n28-Apr-14,43,1,26\n5-May-14,54,1,27\n12-May-14,43,1,24\n19-May-14,65,1,28\n26-May-14,102,1,32\n2-Jun-14,117,1,36\n9-Jun-14,129,1,40\n16-Jun-14,97,1,35\n23-Jun-14,126,1,39\n30-Jun-14,124,1,37\n7-Jul-14,176,1,47\n14-Jul-14,317,1,63\n21-Jul-14,322,1,71\n28-Jul-14,167,1,55\n4-Aug-14,183,1,55\n11-Aug-14,142,1,45\n18-Aug-14,126,1,37\n25-Aug-14,69,1,32\n1-Sep-14,53,1,29\n8-Sep-14,38,1,30\n15-Sep-14,43,1,25\n22-Sep-14,15,1,18\n29-Sep-14,7,1,19\n6-Oct-14,12,1,21\n13-Oct-14,0,1,19\n20-Oct-14,17,1,21\n27-Oct-14,16,1,21\n3-Nov-14,5,1,18\n10-Nov-14,6,1,15\n17-Nov-14,32,1,24\n24-Nov-14,22,1,18\n1-Dec-14,8,1,19\n8-Dec-14,12,1,16\n15-Dec-14,22,1,14\n22-Dec-14,32,1,24\n29-Dec-14,0,2,18\n5-Jan-15,0,2,19\n12-Jan-15,0,2,19\n19-Jan-15,0,2,19\n26-Jan-15,0,2,15\n2-Feb-15,0,2,16\n9-Feb-15,0,2,17\n16-Feb-15,17,2,17\n23-Feb-15,41,2,25\n2-Mar-15,22,2,25\n9-Mar-15,18,2,23\n16-Mar-15,14,2,21\n23-Mar-15,5,2,16\n30-Mar-15,20,2,14\n6-Apr-15,21,2,21\n13-Apr-15,31,2,24\n20-Apr-15,21,2,21\n27-Apr-15,39,2,26\n4-May-15,23,2,31\n11-May-15,67,2,36\n18-May-15,74,2,46\n25-May-15,54,2,41\n1-Jun-15,71,2,49\n8-Jun-15,85,2,46\n15-Jun-15,136,2,65\n22-Jun-15,143,2,85\n29-Jun-15,188,2,95\n6-Jul-15,225,2,92\n13-Jul-15,210,2,94\n20-Jul-15,124,2,81\n27-Jul-15,115,2,62\n3-Aug-15,115,2,56\n10-Aug-15,87,2,48\n17-Aug-15,54,2,32\n24-Aug-15,35,2,40\n31-Aug-15,35,2,32\n7-Sep-15,24,2,29\n14-Sep-15,34,2,23\n21-Sep-15,21,2,20\n28-Sep-15,41,2,29\n5-Oct-15,29,2,26\n12-Oct-15,2,2,17\n19-Oct-15,22,2,20\n26-Oct-15,21,2,21\n2-Nov-15,23,2,23\n9-Nov-15,22,2,25\n16-Nov-15,21,2,18\n23-Nov-15,0,2,16\n30-Nov-15,0,2,17\n7-Dec-15,0,2,16\n14-Dec-15,0,2,17\n21-Dec-15,0,2,15\n28-Dec-15,0,2,16\n4-Jan-16,36,3,19\n11-Jan-16,54,3,22\n18-Jan-16,34,3,20\n25-Jan-16,54,3,23\n1-Feb-16,12,3,15\n8-Feb-16,54,3,21\n15-Feb-16,52,3,21\n22-Feb-16,45,3,17\n29-Feb-16,65,3,21\n7-Mar-16,43,3,18\n14-Mar-16,97,3,28\n21-Mar-16,43,3,19\n28-Mar-16,76,3,28\n4-Apr-16,87,3,25\n11-Apr-16,65,3,25\n18-Apr-16,123,3,27\n25-Apr-16,128,3,28\n2-May-16,32,3,23\n9-May-16,143,3,29\n16-May-16,198,3,42\n23-May-16,322,3,54\n30-May-16,459,3,71\n6-Jun-16,423,3,63\n13-Jun-16,489,3,71\n20-Jun-16,449,3,69\n27-Jun-16,261,3,54\n4-Jul-16,357,3,58\n11-Jul-16,309,3,56\n18-Jul-16,332,3,58\n25-Jul-16,365,3,57\n1-Aug-16,387,3,60\n8-Aug-16,309,3,57\n15-Aug-16,237,3,44\n22-Aug-16,208,3,39\n29-Aug-16,201,3,47\n5-Sep-16,125,3,22\n12-Sep-16,82,3,28\n19-Sep-16,57,3,24\n26-Sep-16,19,3,21\n3-Oct-16,0,3,15\n", "type": "text"}, {"name": "formattools.py", "content": "\"\"\"Format and display functions\"\"\"\n\nimport pandas as pd\n\ndef col_formatting(df):\n    \"\"\"Removes special characters from column names.\n\n    Parameters\n    ----------\n    df: dataframe\n        The dataframe whose columns are to be standardized\n    \n    Returns\n    -------\n    list\n        A list of standarized column names\n    \"\"\"\n    df.columns = df.columns.str.lower()\n    df.columns = df.columns.str.replace(' ', '_')\n    new_cols = []\n    for i in range(0, df.shape[1]):\n        new_col = ''.join([a for a in df.columns[i] if a.isalnum() or a == '_'])\n        new_cols.append(new_col)\n    return new_cols\n\ndef campagin_to_col(df, camp_col: str, media_spend_col: str):\n    \"\"\"Generates individual columns for campaigns.\n\n    Parameters\n    ----------\n    df: dataframe\n        The dataframe whose columns are to be separated\n    camp_col: str\n        The name of the column containing the campaigns\n    media_spend_col:\n        The name of the column including the spending series\n    \n    Returns\n    -------\n    dataframe\n        A dataframe in which each column corresponds to a single campaign\n    \"\"\"\n\n    df = df.join(pd.get_dummies(df[camp_col].apply(str), prefix = 'camp'))\n    for col in [col for col in df.columns if col.startswith('camp_')]:\n        spend_col = col + '_spend'\n        df[spend_col] = df[col] * df[media_spend_col]\n    df.drop([camp_col, media_spend_col], axis = 1, inplace = True)\n    return df", "type": "text"}, {"name": "mediaadtools.py", "content": "\"\"\"Media related tools\"\"\"\n\nimport numpy as np\n\n\nclass CarryOverEffect():\n    \"\"\"Calculates the retention factor.\n\n    Uses convolutions to calculate the retention factor, describing the \n    proportion of media pressure that carries over from one week to the next.\n\n    Attributes\n    ----------\n    spending_series: array\n        An array indicating weekly advertising spend\n    strength: float\n        A float indicating how much gets carried over\n    lenght: int\n        An integer indicating how long does it get carried over\n    \n    Methods\n    -------\n    get_padding_width_per_side()\n        Calculates the number of zeros to add in the pad\n    add_padding_to_array()\n        Adds padding to the array to transform\n    carry_over_pattern()\n        Creates the carry over pattern to transform the spending series\n    convolve()\n        Applies transformation to the spending series\n\n    \"\"\"\n\n\n    def __init__(self, spending_series: np.array, strength: float, lenght: int):\n        \"\"\"\n        Parameters\n        ----------\n        spending_series: array\n            An array indicating weekly advertising spend\n        strength: float\n            A float indicating how much gets carried over\n        lenght: int\n            An integer indicating how long does it get carried over\n        \"\"\"\n        \n        self.spending_series = spending_series\n        self.strength = strength\n        self.lenght = lenght\n\n\n    def get_padding_width_per_side(self) -> int:\n        \"\"\"Calculates the number of zeros to add in the pad.\n\n        Calculates the number of zeros to be added at the beginning and end of \n        the series (pad), to allow the application of convolution.\n\n        Parameters\n        ----------\n        \n\n        Raises\n        ------\n        int\n            The number of zeros to be added in the next step\n        \"\"\"\n\n        if self.lenght >= 1:\n            padding_width = self.lenght - 1\n        else:\n            padding_width = 0\n        return padding_width\n\n\n    def add_padding_to_array(self) -> np.array:\n        \"\"\"Adds padding to the array to transform.\n\n        Adds to the data series to be transformed, the zeros necessary to be \n        able to apply the convolution. Shape: (spending_series + padding_width).\n\n        Parameters\n        ----------\n        \n\n        Raises\n        ------\n        array\n            An array with original values and padding\n        \"\"\"\n        \n        padding_width = self.get_padding_width_per_side()\n        # Multiply with two because we need padding at the beginning and end.\n        # Example, if spending_series.shape = (10, ) and padding = 2, then \n        # spending_series_with_padding.shape = (14, )\n        spending_series_with_padding = np.zeros(\n            self.spending_series.shape[0] + padding_width * 2)\n        spending_series_with_padding[padding_width:-padding_width] = self.spending_series\n        return spending_series_with_padding\n\n\n    def carry_over_pattern(self) -> np.array:\n        \"\"\"Creates the carry over pattern to transform the spending series.\n\n        It generates an array showing the decreasing evolution of the Retention \n        Factor over the weeks. The values that conform it are between (0, 1].\n\n        Parameters\n        ----------\n        \n\n        Raises\n        ------\n        array\n            An array of values between (0, 1]\n        \"\"\"\n        pattern = []\n        for i in range(0, self.lenght):\n            multiplier = self.strength ** (self.lenght - 1 - i)\n            pattern.append(multiplier)\n        return np.array(pattern)\n\n\n    def convolve(self) -> np.array:\n        \"\"\"Applies transformation to the spending series.\n\n        Uses the carry over pattern to transform the spending series by applying\n        a convolution to simulate the Retention Factor over the weeks.\n\n        Parameters\n        ----------\n        \n\n        Raises\n        ------\n        array\n            A transformed array\n        \"\"\"\n\n        spending_series_with_padding = self.add_padding_to_array()\n        spending_series_size = self.spending_series.shape[0]\n        pattern = self.carry_over_pattern()\n        pattern_size = pattern.shape[0]\n        convolved_sepending_series = np.zeros(spending_series_size)\n        for i in range(0, spending_series_size):            \n            convolved_sepending_series[i] = np.sum(np.multiply(\n                spending_series_with_padding[i: i + pattern_size], pattern))\n        return convolved_sepending_series", "type": "text"}, {"name": "model_1.py", "content": "from modeltools import cross_validation_table\nfrom sklearn.linear_model import LinearRegression\nfrom formattools import col_formatting, campagin_to_col\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom mediaadtools import CarryOverEffect\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n\nclass MediaModel():\n    \"\"\"...\n\n    ...\n\n    Attributes\n    ----------\n    strength: float\n        A float indicating how much gets carried over\n    lenght: int\n        An integer indicating how long does it get carried over\n    a: int\n        A non-negative integer used to simulate saturation \n    \n    Methods\n    -------\n    prepare_data()\n        Prepares the data to be able to apply the models\n    get_crossval_table()\n        Create a cross validation summary and measures means\n    make_fit_plot()\n        Generates chart showing the model fit\n    predict()\n        Generates prediction for the data\n    make_fit_plot()\n        Generates chart showing the model fit\n    contribution_plot()\n        Generates graphs showing the adjusted contribution of each campaign\n    \"\"\"\n\n\n    def __init__(self, strength: float, lenght: int, a: int):\n        \"\"\"\n        Parameters\n        ----------\n        strength: float\n            A float indicating how much gets carried over\n        lenght: int\n            An integer indicating how long does it get carried over\n        a: int\n            A non-negative integer used to simulate saturation \n        \"\"\"\n\n        self.strength = strength\n        self.lenght = lenght\n        self.a = a      \n\n\n    def prepare_data(self):\n        \"\"\"Prepares the data to be able to apply the models.\n\n        Prepares the data to be consumed by the model. Standardize column names,\n        separate campaigns into individual variables, apply transformations to \n        expenditures (carry over effect and saturation).\n\n        Parameters\n        ----------\n        \n\n        Raises\n        ------\n        dataframe\n            A dataframe with original and transformed columns\n        X\n            A dataframe with transformed columns (predictive variables)\n        y\n            An array with sales series (target)\n        \"\"\"\n        #infile = 'data.csv'\n        infile = Path(__file__).parent / 'data.csv'\n        df = pd.read_csv(infile, \n                        parse_dates = ['Date (Week)'],\n                        index_col = 'Date (Week)')\n        df.columns = col_formatting(df)\n        df = campagin_to_col(df, 'media_campaign', 'media_spend_usd')\n        # Carry Over Effect transformation\n        for col in ['camp_1_spend', 'camp_2_spend', 'camp_3_spend']:\n            spend_col = col + '_adstock'\n            df[spend_col] = CarryOverEffect(df[col], self.strength, \n                                        self.lenght).convolve()\n        X = df.drop(columns = ['search_volume', 'camp_1', 'camp_2', 'camp_3', \n                                'camp_1_spend', 'camp_2_spend', 'camp_3_spend'])\n        y = df['search_volume']\n        # Saturation transformation\n        # Applies an exponential function to transform the spending series and \n        # to simulate saturation. It's monotonically increasing and it takes \n        # values between (0, 1].\n        #X = 1 - np.exp(-self.a * X)\n        return df, X, y\n\n\n    #def find_hyperparm(self, X, y, model):\n        \"\"\"...\n\n        ...\n\n        Parameters\n        ----------\n        \n\n        Raises\n        ------\n        ...\n            ...\n        ...\n            ...\n        ...\n            ...\n        \"\"\"\n        \n        #return df, X, y\n\n    \n    def get_crossval_table(self, X, y, model = LinearRegression()):\n        \"\"\"Create a cross validation summary and measures means.\n        \n        Generates dataframe with the results obtained in each fold during \n        cross validation process. It calculates the k-fold  and k-1 fold means.\n\n        Parameters\n        ----------\n        X: dataframe\n            A dataframe with transformed columns (predictive variables)\n        y: array\n            An array with sales series (target)\n        model: sklearn model, optional\n            sklearn model, default = LinearRegression()\n\n        Raises\n        ------\n        cv_df: dataframe\n            A dataframe containing cross validation summary\n        cv_r2_mean: float\n            A float indicating the mean of the k-folds (r2)\n        cv_n_1_r2_mean: float\n            A float indicating the mean of the k-1-folds (r2)\n        \"\"\"\n        # We do not use the standard k-fold cross-validation here because we are\n        # dealing with time series data.\n        ts_cv = TimeSeriesSplit(n_splits = 4)\n        cv_df = cross_validation_table(X, y, model, ts_cv)\n        cv_r2_mean = cv_df['test_r2'].mean()\n        cv_n_1_r2_mean = cv_df.iloc[:-1]['test_r2'].mean()\n        return cv_df, cv_r2_mean, cv_n_1_r2_mean\n\n\n    def predict(self, X, y):\n        \"\"\"Generates prediction for the data.\n        \n        Generates the prediction based in X and y, the model is \n        LinearRegression().\n\n        Parameters\n        ----------\n        X: dataframe\n            A dataframe with transformed columns (predictive variables)\n        y: array\n            An array with sales series (target)\n        model: sklearn model, optional\n            sklearn model, default = LinearRegression()\n\n        Raises\n        ------\n        coef: array\n            Float array containing the regression coefficients\n        intercept: float\n            A float corresponding to the intercept of the regression\n        y_pred: float\n            An array with predicted sales series\n        \"\"\"\n        lr = LinearRegression()\n        lr.fit(X, y) # refit the model with the complete dataset\n        coef = lr.coef_\n        intercept = lr.intercept_\n        y_pred = lr.predict(X)\n        return coef, intercept, y_pred\n\n    \n    def make_fit_plot(self, X, y, y_pred):\n        \"\"\"Generates chart showing the model fit.\n\n        Plots the observed values together with the values obtained by the \n        model.\n\n        Parameters\n        ----------\n        X: dataframe\n            A dataframe with transformed columns (predictive variables)\n        y: array\n            An array with sales series (target)\n        y_pred: array\n            An array with predicted sales series\n\n        Raises\n        ------\n        fig\n            A chart showing the model fit\n        \"\"\"\n        fig, ax = plt.subplots(figsize = (16, 10), layout = 'constrained')\n        ax.plot(X.index, y, label='Observed') \n        ax.plot(X.index, y_pred, label='Predicted') \n        ax.set_xlabel('Week') \n        ax.set_ylabel('Google Search volumes') \n        ax.set_title('Model fitting (Historical data vs. Predictions)') \n        ax.legend()\n        return fig\n\n    def calculate_roi(self, X, y, coef, intercept):\n        weights = pd.Series(coef, index = X.columns)\n        base = intercept\n        unadj_contributions = X.mul(weights).assign(Base = base)\n        adj_contributions = (unadj_contributions\n                            .div(unadj_contributions.sum(axis = 1), axis = 0)\n                            .mul(y, axis = 0)\n                            ) # contains all contributions for each day\n        return adj_contributions\n\n    def roi_table(self, df, adj_contributions):\n        roi_df = pd.DataFrame(columns = ['campaign_n', 'searches_from_camp_n', 'spendings_on_camp_n', 'roi'])\n        for i in range(1, 4):\n            col1 = 'camp_'+ str(i) +'_spend_adstock'\n            col2 = 'camp_'+ str(i) +'_spend'\n            new_row = pd.Series({'campaign_n': i, \n                                'searches_from_camp_n': adj_contributions[col1].sum(),\n                                'spendings_on_camp_n': df[col2].sum(), \n                                'roi': adj_contributions[col1].sum() / df[col2].sum()})\n            roi_df = pd.concat([roi_df, new_row.to_frame().T], ignore_index = True)\n            roi_df['1/roi'] = 1 / roi_df['roi']\n        return roi_df\n    \n    def contribution_plot(self, adj_contributions):\n        \"\"\"Generates graphs showing the adjusted contribution of each campaign.\n\n        Graph the contribution of each campaign to the search volume for the \n        period. Displays the campaigns and the base in different colors.\n\n        Parameters\n        ----------\n        adj_contributions: dataframe\n            A dataframe with the adjusted contributions for each campaign \n\n        Raises\n        ------\n        fig\n            A chart showing adjusted contributions and base\n        \"\"\"\n        ax = (adj_contributions[['Base', 'camp_1_spend_adstock', \n                                'camp_2_spend_adstock', 'camp_3_spend_adstock']]\n        .plot.area(\n            figsize = (16, 10),\n            linewidth = 1,\n            title = 'Predicted Searches and Breakdown',\n            ylabel = 'Sales',\n            xlabel = 'Date')\n                )\n        ax.set_xlabel('Week') \n        ax.set_ylabel('Google Search volumes') \n        ax.set_title('Actual vs Pred') \n        ax.legend()\n        return plt.figure()", "type": "text"}, {"name": "modeltools.py", "content": "\"\"\"Model related tools\"\"\"\nfrom sklearn.model_selection import cross_validate, TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nimport numpy as np\n\ndef cross_validation_table(X, y: np.array, model = LinearRegression(), \n                            cross_val = TimeSeriesSplit()):\n    \"\"\"Generates dataframe with the results obtained in each fold during cross \n    validation process.\n\n    Parameters\n    ----------\n    X: dataframe\n        The dataframe whose columns are to be standardized\n    y: array\n        An array with sales series (target)\n    model: sklearn model, optional\n        sklearn model, default = LinearRegression()\n    cross_val: sklearn object, optional\n        Sample split technique\n    \n    Returns\n    -------\n    dataframe\n        A dataframe with the results obtained during cross validation\n    \"\"\"\n    cv_df = pd.DataFrame(columns = ['fold', 'X_train_index', 'X_train_dates', \n                        'X_test_index', 'X_test_dates'])\n    for i, (train_index, test_index) in enumerate(cross_val.split(X)):\n        fold = i\n        train_ind_init = str(train_index[0])\n        train_ind_fin = str(train_index[-1])\n        test_ind_init = str(test_index[0])\n        test_ind_fin = str(test_index[-1])\n        train_dates_init = str(X.index[train_index[0]]).replace('00:00:00', '')\n        train_dates_fin = str(X.index[train_index[-1]]).replace('00:00:00', '')\n        test_dates_init = str(X.index[test_index[0]]).replace('00:00:00', '')\n        test_dates_fin = str(X.index[test_index[-1]]).replace('00:00:00', '')\n        new_row = pd.Series({'fold': fold, \n                    'X_train_index': train_ind_init + ' - ' + train_ind_fin, \n                    'X_train_dates': train_dates_init + ' - ' + train_dates_fin, \n                    'X_test_index': test_ind_init + ' - ' + test_ind_fin, \n                    'X_test_dates': test_dates_init + ' - ' + test_dates_fin})\n        cv_df = pd.concat([cv_df, new_row.to_frame().T], ignore_index = True)        \n        cv_results = cross_validate(model, X, y, cv = cross_val, \n                                scoring = ('r2', 'neg_mean_squared_error'))\n        df = cv_df.join(pd.DataFrame(cv_results))\n    return df\n\n\n\n\ndef find_hyperparams(X, y, model = Lasso(), cross_val = TimeSeriesSplit()):\n    param_grid = {\n        \"alpha\": [x for x in range(1, 100)] + [y/10 for y in range(10)],\n        \"tol\": [0.00001, 0.0000001, 0.01],\n        \"selection\": ['cyclic', 'random']\n        }\n    clf = GridSearchCV(model, param_grid, cv = cross_val, error_score = -1000, \n    n_jobs = -1, scoring = 'r2')\n    clf.fit(X, y)\n    print(\"Best score: \" + str(clf.best_score_))\n    summary = pd.DataFrame(clf.cv_results_)\n    summary.sort_values(by = 'rank_test_score')\n    dc_scores = {}\n    dc_scores_esc = {}\n    dc_scores[str(model).split('(')[0]] = {'model': clf.best_estimator_, \n    'score': clf.best_score_}\n    return dc_scores, dc_scores_esc, summary", "type": "text"}, {"name": "requirements.txt", "content": "//5hAHMAdAB0AG8AawBlAG4AcwA9AD0AMgAuADIALgAxAA0ACgBiAGEAYwBrAGMAYQBsAGwAPQA9ADAALgAyAC4AMAANAAoAYwBvAGwAbwByAGEAbQBhAD0APQAwAC4ANAAuADYADQAKAGMAbwBtAG0APQA9ADAALgAxAC4AMgANAAoAYwBvAG4AdABvAHUAcgBwAHkAPQA9ADEALgAwAC4ANwANAAoAYwB5AGMAbABlAHIAPQA9ADAALgAxADEALgAwAA0ACgBkAGUAYgB1AGcAcAB5AD0APQAxAC4ANgAuADYADQAKAGQAZQBjAG8AcgBhAHQAbwByAD0APQA1AC4AMQAuADEADQAKAGUAeABlAGMAdQB0AGkAbgBnAD0APQAxAC4AMgAuADAADQAKAGYAbwBuAHQAdABvAG8AbABzAD0APQA0AC4AMwA4AC4AMAANAAoAaQBwAHkAawBlAHIAbgBlAGwAPQA9ADYALgAyADEALgAxAA0ACgBpAHAAeQB0AGgAbwBuAD0APQA4AC4AOQAuADAADQAKAGoAZQBkAGkAPQA9ADAALgAxADgALgAyAA0ACgBqAG8AYgBsAGkAYgA9AD0AMQAuADIALgAwAA0ACgBqAHUAcAB5AHQAZQByAF8AYwBsAGkAZQBuAHQAPQA9ADgALgAwAC4AMgANAAoAagB1AHAAeQB0AGUAcgBfAGMAbwByAGUAPQA9ADUALgAyAC4AMAANAAoAawBpAHcAaQBzAG8AbAB2AGUAcgA9AD0AMQAuADQALgA0AA0ACgBtAGEAdABwAGwAbwB0AGwAaQBiAD0APQAzAC4ANgAuADMADQAKAG0AYQB0AHAAbABvAHQAbABpAGIALQBpAG4AbABpAG4AZQA9AD0AMAAuADEALgA2AA0ACgBuAGUAcwB0AC0AYQBzAHkAbgBjAGkAbwA9AD0AMQAuADUALgA2AA0ACgBuAHUAbQBwAHkAPQA9ADEALgAyADQALgAyAA0ACgBwAGEAYwBrAGEAZwBpAG4AZwA9AD0AMgAzAC4AMAANAAoAcABhAG4AZABhAHMAPQA9ADEALgA1AC4AMwANAAoAcABhAHIAcwBvAD0APQAwAC4AOAAuADMADQAKAHAAaQBjAGsAbABlAHMAaABhAHIAZQA9AD0AMAAuADcALgA1AA0ACgBQAGkAbABsAG8AdwA9AD0AOQAuADQALgAwAA0ACgBwAGwAYQB0AGYAbwByAG0AZABpAHIAcwA9AD0AMgAuADYALgAyAA0ACgBwAHIAbwBtAHAAdAAtAHQAbwBvAGwAawBpAHQAPQA9ADMALgAwAC4AMwA2AA0ACgBwAHMAdQB0AGkAbAA9AD0ANQAuADkALgA0AA0ACgBwAHUAcgBlAC0AZQB2AGEAbAA9AD0AMAAuADIALgAyAA0ACgBQAHkAZwBtAGUAbgB0AHMAPQA9ADIALgAxADQALgAwAA0ACgBwAHkAcABhAHIAcwBpAG4AZwA9AD0AMwAuADAALgA5AA0ACgBwAHkAdABoAG8AbgAtAGQAYQB0AGUAdQB0AGkAbAA9AD0AMgAuADgALgAyAA0ACgBwAHkAdAB6AD0APQAyADAAMgAyAC4ANwAuADEADQAKAHAAeQB3AGkAbgAzADIAPQA9ADMAMAA1AA0ACgBwAHkAegBtAHEAPQA9ADIANQAuADAALgAwAA0ACgBzAGMAaQBrAGkAdAAtAGwAZQBhAHIAbgA9AD0AMQAuADIALgAxAA0ACgBzAGMAaQBwAHkAPQA9ADEALgAxADAALgAwAA0ACgBzAGkAeAA9AD0AMQAuADEANgAuADAADQAKAHMAdABhAGMAawAtAGQAYQB0AGEAPQA9ADAALgA2AC4AMgANAAoAdABoAHIAZQBhAGQAcABvAG8AbABjAHQAbAA9AD0AMwAuADEALgAwAA0ACgB0AG8AcgBuAGEAZABvAD0APQA2AC4AMgANAAoAdAByAGEAaQB0AGwAZQB0AHMAPQA9ADUALgA5AC4AMAANAAoAdwBjAHcAaQBkAHQAaAA9AD0AMAAuADIALgA2AA0ACgA=", "type": "binary"}]